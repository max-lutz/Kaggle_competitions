{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import r2_score, max_error, mean_absolute_error, mean_squared_error, mean_squared_log_error\n\nfrom sklearn.model_selection import train_test_split\n\nsns.set_style('darkgrid')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()\nprint(\"-\"*40)\ndf_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def columns_with_missing_data(df, threshold = 10):\n    empty = df.isnull().sum()\n    drop_list = []\n    for i in range(empty.size):\n        if empty[i] > threshold:\n            drop_list.append(df.columns[i])\n    return drop_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_missing_data(df):\n    df = df.fillna(df.median())\n    empty = df.isnull().sum()\n    drop_list = []\n    for i in range(empty.size):\n        if empty[i] > threshold:\n            drop_list.append(df.columns[i])\n    return drop_list\n\n#df = df.fillna(df.mode().iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we get a list of column with more than 25 missing values\ndrop_list_train = columns_with_missing_data(df_train, 25)\nprint(drop_list_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns = drop_list_train)\ndf_test = df_test.drop(columns = drop_list_train)\nprint(columns_with_missing_data(df_train, 25))\nprint(columns_with_missing_data(df_test, 25))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have no more column with more than 25 missing values. <br>\nNow let's fill the missing values with median values for each column","metadata":{}},{"cell_type":"code","source":"print(columns_with_missing_data(df_train, 0))\nprint(columns_with_missing_data(df_test, 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.fillna(df_train.median())\ndf_test = df_test.fillna(df_train.median())\n\ndf_train = df_train.fillna(df_train.mode().iloc[0])\ndf_test = df_test.fillna(df_train.mode().iloc[0])\n\nprint(columns_with_missing_data(df_train, 0))\nprint(columns_with_missing_data(df_test, 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in df_train.columns:\n    if(df_train[feature].dtype == np.object):\n        print(feature, '   ', df_train[feature].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_train = df_train.corr().iloc[[-1]]\ncorr_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_col_list = list(df_train.select_dtypes(include=['object']).columns)\nfor i in encode_col_list:\n    df_train = pd.concat([df_train,pd.get_dummies(df_train[i], prefix=i)],axis=1)\n    df_train.drop(i, axis = 1, inplace=True)\n    \nencode_col_list = list(df_test.select_dtypes(include=['object']).columns)\nfor i in encode_col_list:\n    df_test = pd.concat([df_test,pd.get_dummies(df_test[i], prefix=i)],axis=1)\n    df_test.drop(i, axis = 1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_train = df_train.corr().iloc[[df_train.columns.get_loc(\"SalePrice\")]]\ncorr_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the name of all the features where the correlation with the price is between -0.5 and 0.5 \ndrop_feature = corr_train[abs(corr_train[corr_train.columns]) < 0.5].dropna(axis='columns').columns\ndrop_feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=drop_feature, errors='ignore')\ndf_test = df_test.drop(columns=drop_feature, errors='ignore')\ncorr_train = df_train.corr().iloc[[df_train.columns.get_loc(\"SalePrice\")]]\ncorr_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop(columns=['SalePrice'])\ny = np.array([df_train['SalePrice']]).T\n\nX_pred = df_test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor_linear_regression = LinearRegression()\nregressor_linear_regression.fit(X_train, y_train)\n\ny_pred_linear_regression = regressor_linear_regression.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#poly_reg = PolynomialFeatures(degree = 4)\n#X_poly = poly_reg.fit_transform(X_train)\n#regressor_polynomial_regression = LinearRegression()\n#regressor_polynomial_regression.fit(X_poly, y_train)\n\n#y_pred_polynomial_regression = regressor_polynomial_regression.predict(poly_reg.transform(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc_X = StandardScaler()\nsc_y = StandardScaler()\ny_train_reshaped = y_train.reshape(len(y_train),1)\nX_train_scaled = sc_X.fit_transform(X_train)\ny_train_scaled = sc_y.fit_transform(y_train_reshaped)\n\nregressor_svr = SVR(kernel = 'rbf')\nregressor_svr.fit(X_train_scaled, y_train_scaled)\n\ny_pred_svr = sc_y.inverse_transform(regressor_svr.predict(sc_X.transform(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor_decision_tree = DecisionTreeRegressor(random_state = 0)\nregressor_decision_tree.fit(X_train, y_train)\n\ny_pred_decision_tree = regressor_decision_tree.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor_random_forest = RandomForestRegressor(n_estimators = 500, random_state = 0)\nregressor_random_forest.fit(X_train, y_train)\n\ny_pred_random_forest = regressor_random_forest.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [['Multiple linear regression', r2_score(y_test, y_pred_linear_regression), max_error(y_test, y_pred_linear_regression), mean_absolute_error(y_test, y_pred_linear_regression), mean_squared_error(y_test, y_pred_linear_regression), mean_squared_log_error(y_test, y_pred_linear_regression)],\n#['Polynomial regression', r2_score(y_test, y_pred_polynomial_regression), max_error(y_test, y_pred_polynomial_regression), mean_absolute_error(y_test, y_pred_polynomial_regression), mean_squared_error(y_test, y_pred_polynomial_regression), mean_squared_log_error(y_test, y_pred_polynomial_regression)],\n['Support vector regression', r2_score(y_test, y_pred_svr), max_error(y_test, y_pred_svr), mean_absolute_error(y_test, y_pred_svr), mean_squared_error(y_test, y_pred_svr), mean_squared_log_error(y_test, y_pred_svr)],\n['Decision tree', r2_score(y_test, y_pred_decision_tree), max_error(y_test, y_pred_decision_tree), mean_absolute_error(y_test, y_pred_decision_tree), mean_squared_error(y_test, y_pred_decision_tree), mean_squared_log_error(y_test, y_pred_decision_tree)],\n['Random forest regression', r2_score(y_test, y_pred_random_forest), max_error(y_test, y_pred_random_forest), mean_absolute_error(y_test, y_pred_random_forest), mean_squared_error(y_test, y_pred_random_forest), mean_squared_log_error(y_test, y_pred_random_forest)]] \nheaders_1=[\"1\", \"2\", \"3\", \"4\"] \nheaders_2=[\"Algorithm\", \"RÂ² score\", \"Max err\", \"Mean Abs err\", \"mean sqr err\", 'root mean sqr log err'] \nprint(pd.DataFrame(data, headers_1, headers_2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We select the random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 600, num = 3)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors / test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 0)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)\n\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rf_random.predict(X_pred)\n\noutput = pd.DataFrame({'Id': pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\").Id, 'SalePrice': y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}